---
layout: documentation
title: "RethinkDB FAQ for programmers new to distributed databases"
short_title: Architecture FAQ
active: docs
docs_active: architecture
permalink: docs/architecture/
alias: docs/advanced-faq/
js: faq_index
---

{% infobox info %}
<strong>Want to learn more about the basics?</strong>

* Read the <a href="/docs/guide/javascript/">ten-minute guide</a> to get started with using RethinkDB.
* <a href="/faq/">Read the FAQ</a> for programmers new to distributed systems.
* Jump into the [cookbook](/docs/cookbook/javascript/) and see dozens of examples of common RethinkDB queries.
{% endinfobox %}

<div id="faqcontents"></div>
---
{% faqsection Sharding and replication %}

## How does RethinkDB partition data into shards? ##

RethinkDB uses a range sharding algorithm parameterized on the table's
primary key to partition the data. When the user states they want a
given table to use a certain number of shards, the system examines the
statistics for the table and finds the optimal set of split points to
break up the table evenly. All sharding is currently done based on the
table's primary key, and cannot be done based on any other attribute
(in RethinkDB the primary key and the shard key are effectively the
same thing).

For example, if a given table contains a thousand JSON documents whose
primary keys are uniformly distributed, alphabetical, upper-case strings and the user states
they want two shards, RethinkDB will likely pick split point 'M' to
partition the table. Every document with a primary key less than or
equal to 'M' will go into the first shard, and every document with a
primary key greater than 'M' will go into the second shard. The split
point will be picked such that each shard contains close to five
hundred keys, and the shards will automatically be distributed across
the cluster.

Even if the primary keys contain unevenly distributed data (such as
human last names, where some keys are likely to occur much more
frequently than others), the system will still pick a correct split
point to ensure that each shard has a roughly similar number of
documents (there are many more Smiths in the phone book than
Akhmechets).

In advanced situations, the user can specify the split points
manually, and the system will partition the table into shards
according to the user's request. When the split points for the table
change, RethinkDB uses a bit of math to figure out the optimal way to
copy as little data as possible. For example, if the split point for a
table used to be 'M', and is later changed to 'O', RethinkDB will
figure out that it only needs to copy the data between keys 'M' and
'O' from one shard to another.

Internally this approach is more difficult to implement than the more
commonly used consistent hashing, but it has significant advantages
because it allows for an efficient implementation of range queries.

## What governs the location of shards and replicas in the cluster? ##

RethinkDB's clustering layer is based on three concepts: goals,
directory, and blueprints.

The _goals_ are what the user (or an automated script) specifies to the
clustering system via the web UI or the command line tools. For
example, the user might specify that a table is to be broken up into
five shards based on specific split points, that each shard is to be
replicated five times within the cluster, and that at least two of
those replicas are to be located in a European datacenter.

The _directory_ is the current physical state of the cluster&mdash; how many
machines are accessible, what data is currently stored on each
machine, etc. The data structures that keep track of the directory are
automatically updated when the cluster changes. For example, if a
machine dies due to power failure, the directory is updated to
represent this change.

The _blueprint_ is a data structure automatically generated by RethinkDB
to satisfy the user's goals based on the constraints imposed by the
physical reality. It contains information about which data each
machine in the cluster is to be responsible for. For example, if the
user updates the goals to state that the European datacenter is to
contain three instead of two replicas, the system will pick an extra
machine in the European datacenter to be responsible for an additional
replica, update the blueprint with that information, and propagate it
to the rest of the cluster.

Each machine in the cluster monitors the part of the blueprint it's
responsible for, and constantly tries to match the directory to the
blueprint. For example, if the blueprint states that machine M is
responsible for a particular shard, but the machine M doesn't have
this shard, the machine will automatically react to copy the data from
one of the available replicas, and will take appropriate steps to keep
it up to date.

RethinkDB uses a set of heuristics to attempt to satisfy the user's
goals in an optimal way. It will try to copy data for new replicas
from the closest available machine, evenly distribute replicas of the
data across the cluster, try to place burden on machines that aren't
dealing with heavy load, etc.

In advanced situations where these heuristics aren't sufficient, the user can
include specific hardware requirements into the goals. For example, the user
can specify that a given replica is to be located on a specific machine in the
cluster, and RethinkDB will generate a blueprint to satisfy these requirements.

## How does multi-datacenter support work? ##

To take advantage of availability zone support, you first have to
group machines into datacenters. Once you tell RethinkDB which
datacenters machines in the cluster belong to, you can define
replication and acknowledgement settings on per-datacenter basis.

For example, you may specify that a table should have at least three
replicas in the US\_East datacenter, and at least five replicas in
US\_West. You also might specify that before a write is acknowledged to
the client, at least two replicas in each datacenter must acknowledge
the write.

Once you set replication and acknowledgement goals on a per-datacenter
basis, the system takes care of data replication and enforcing write
acknowledgement counts automatically.

RethinkDB uses the same protocol for communication within a datacenter
as it does across datacenters. Because the architecture is immediately
consistent and does not require quorums on individual document reads
and writes, the latency issues commonly associated with
cross-datacenter quorums on Dynamo-style systems do not arise in
RethinkDB.

## Does RethinkDB automatically reshard the database without the user's request? ##

The short answer is no. The longer answer is that the clustering
system is designed with three main principles in mind:

- Common operations such as scaling up and down, rebalancing shards,
  and increasing/decreasing replication count should easily be
  performed in a click of a button.
- In cases where it matters, the system should give administrators
  fine-tuned control, such as pinning specific masters and replicas to
  specific machines in the cluster.
- Information about the cluster and all operations on the cluster
  should be programmatically accessible.

We felt that performing automatic maintenance operations on the
cluster (such as adding shards) is a higher-level component, and that
it's crucial to have a really good implementation of the lower-level
components done first. As a result, the clustering system is organized
into three layers:

- The first layer implements the distributed infrastructure, placing
  copies of data on specific machines, routing queries, etc.
- The second layer builds on the first and implements various
  automation mechanisms (e.g. automatically determining how to split
  shards, where to place copies of the data, automatically picking
  optimal masters, etc.)  This is the layer that compiles goals
  specified by the user into blueprints.
- The third layers builds on the previous two and provides the user
  with command line and web-based tools to control the cluster.

Invoking this functionality automatically without the user's request
is the next layer in this hierarchy. Currently the user can control
the system via the web UI, manually via the command line, or by
writing scripts to call the command line tools to perform server
automation.

We're exploring best practices to determine whether it's possible to
build a really good general purpose automation layer that controls the
cluster by automatically enforcing user-specified rules (such as
resharding the system when the shard balance drops below a certain
threshold).

{% endfaqsection %}

{% faqsection CAP theorem %}

## Is RethinkDB immediately or eventually consistent? ##

In RethinkDB data always remains immediately consistent and
conflict-free, and a read that follows a write is always guaranteed to
see the write. This is accomplished by always assigning every shard to
a single authoritative master. All reads and writes to any key in a
given shard always get routed to its respective master where they're
ordered and evaluated.

RethinkDB supports both up-to-date and out-of-date reads. By default,
all read queries are executed up-to-date, which means that every read
operation for a given shard is routed to the master for that shard and
executed in order with other operations on the shard. In this default
mode, the client always sees the latest, consistent, artifact-free
view of the data.

The programmer can also mark a read query to be ok with out-of-date
data. In this mode, the query isn't necessarily routed to the shards
master, but is likely to be routed to its closest replica. Out-of-date
queries are likely to have lower latency and have stronger
availability guarantees, but don't necessarily return the latest
version of the data to the client.

## What CAP theorem tradeoffs are made in RethinkDB? ##

The essential tradeoff exposed by the CAP theorem is this: in case of
network partitioning, does the system maintain availability or data
consistency? (Jumping ahead, RethinkDB chooses to maintain data
consistency).

Dynamo-based systems such as Cassandra and Riak choose to maintain
stronger availability. In these systems if there is a network
partition, the clients can write to the same row on both sides of the
netsplit. In exchange for the write availability, applications built
on top of these systems must deal with various complexities such as
clock skew, conflict resolution code, conflict repair operations,
performance issues for highly contested keys, and latency issues
associated with quorums.

Authoritative systems, such as RethinkDB and MongoDB choose to
maintain data consistency. Building applications on top of
authoritative-master systems is much simpler because all of the issues
associated with data inconsistency do not arise. In exchange, these
applications will occasionally experience availability issues.

In RethinkDB, if there is a network partition, the behavior of the
system from any given client's perspective depends on which side of
the netsplit that client is on. If the client is on the same side of
the netsplit as the master for the shard the client is trying to
reach, it will continue operating without any problems. If the client
is on the opposite side of the netsplit from the master for the shard
the client is trying to reach, the client's up-to-date queries and
write queries will encounter a failure of availability. For example,
if the client is running an up-to-date range query that spans multiple
shards, the masters for all shards must be on the same side of the
netsplit as the client, or the client will encounter a failure of
availability.

If the programmer marks a read query to be ok with out-of-date data,
RethinkDB will route the query to the closest available replica
instead of routing it to the master. In this case the client will see
the data as long as there are replicas of the data on the its side of
the netsplit. However, in this case the data has the risk of being out
of date. This is usually ok for reports, analytics, cached data, or
any scenario in general where having the absolute latest information
isn't imperative.

## How is cluster configuration propagated? ##

Updating the state of a the cluster is a surprisingly difficult
problem in distributed systems. At any given point different (and
potentially) conflicting configurations can be selected on different
sides of a netsplit, different configurations can reach different
nodes in the cluster at unpredictable times, etc.

RethinkDB uses semilattices to store and propagate cluster
configuration (including goals and blueprints). Various parts of the
semilattices are versioned via internal vector clocks. This
architecture turns out to have sufficient mathematical properties to
address all the issues mentioned above (this result has been known in
distributed systems research for quite a while).

{% endfaqsection %}

{% faqsection Indexing %}

## How does RethinkDB index data? ##

When the user creates a table, they have the option of specifying the
attribute that will serve as the primary key (if the primary key
attribute isn't specified, it defaults to 'id'). When the user inserts
a document into the table, if the document contains the primary key
attribute, its value is used to index the document. Otherwise, a random
unique ID is generated for the index automatically.

The primary key of each document is used by RethinkDB to place the
document into an appropriate shard, and index it within that shard
using a B-Tree data structure. Querying documents by primary key is
extremely efficient, because the query can immediately be routed to
the right shard and the document can be looked up in the B-Tree.

## Does RethinkDB support secondary and compound indexes? ##

RethinkDB supports both secondary and compound indexes, as well as
indexes that compute arbitrary expressions. You
can see examples of how to use the secondary index API
[here](/docs/secondary-indexes).

{% endfaqsection %}

{% faqsection Availability and failover %}
## What happens when a machine becomes unreachable? ##

The first thing that happens when a node in the cluster becomes
unreachable is that an issue is raised by the RethinkDB cluster. It is
immediately and prominently displayed in the WebUI, and is accessible
via the command line administration tools.

While the machine remains unreachable, table availability is
determined by the following three cases:

- If the machine is acting as a master for any shards, the
  corresponding tables lose read and write availability (out-of-date
  reads remain possible as long as there are other replicas of the
  shards).
- If the machine is acting as a replica for a given table and there
  aren't enough replicas in the cluster to respect the user's write
  acknowledgement settings, the table loses write availability (but
  maintains read availability). 
- If the machine isn't acting as a master for any tables, and there
  are enough replicas to respect the user's write acknowledgement
  settings, the system continues operating as normal.

There are two possible solutions to this issue. The first option is to
simply wait for the machine to become reachable again. If the machine
comes back up RethinkDB automatically performs the following actions
without any user interaction: replicas on the machine are brought up
to date with the latest changes, masters on the machine become active
again, the cluster clears the reachability issue from web and command
line tools, availability is restored, and the cluster continues
operating as normal.

The second option is to declare the machine dead. If the machine is
declared dead, it is absolved of all responsibilities, and one of the
replicas is automatically elected to act as a new master. After the
machine is declared dead, availability is quickly restored and the
cluster begins operating normally. (If the dead machine comes back up,
it is rejected by the cluster as a zombie).

Currently, RethinkDB does not automatically declare machines dead
after a timeout. This can be done by the user, either in the web UI,
manually via the command line, or by scripting the command line tools.

## What are availability and performance impacts of sharding and replication? ##

RethinkDB maintains availability if the user increases or decreases
the number of replicas in the cluster. In most cases, the replication
process should not have a strong performance impact on the real-time
system.

RethinkDB may or may not maintain availability if the user modifies
the number of shards. In many cases availability will be maintained,
but currently it cannot be guaranteed. We're exploring different
solutions to remove this limitation.

{% endfaqsection %}

{% faqsection Query execution %}

## How does RethinkDB execute queries? ##

When a node in the cluster receives a query from the client, it
evaluates the query in the following way.

First, the query is transformed into an execution plan that consists
of a stack of internal logical operations. The operation stack fully
describes the query in a data structure useful for efficient
execution. The bottom-most node of the stack usually deals with data
access&mdash; it can be a lookup of a single document, a short range scan
bounded by an index, or even a full table scan. Nodes closer to the
top usually perform transformations on the data -- mapping the values,
running reductions, grouping, etc. Nodes can be as simple as
projections (i.e. returning a subset of the document), or as complex
as entire stacks of stacks in case of subqueries.

Each node in the stack has a number of methods defined on it. The
three most important methods define how to execute a subset of the
query on each machine in the cluster, how to combine the data from
multiple machines into a unified resultset, and how to stream data to
the nodes further up in small chunks.

As the client attempts to stream data from the server, these stacks
are transported to every relevant machine in the cluster, and each
machine begins evaluating the topmost node in the stack, in parallel
with other machines. On each machine, the topmost node in the stack
grabs the first chunk of the data from the node below it, and applies
its share of transformations to it. This process proceeds recursively
until enough data is collected to send the first chunk to the
client. The data from each machine is combined into a single
resultset, and forwarded to the client. This process continues as the
client requests more data from the server.

The two most important aspects of the execution engine is that every
query is completely parallelized across the cluster, and that queries
are evaluated lazily. For instance, if the client requests only one
document, RethinkDB will try to do just enough work to return this
document, and will not process every shard in its entirety. This
allows for large, complicated queries to execute in a very efficient
way.

The full query execution process is fairly complex and nuanced. For
example, some operations cannot be parallelized, some queries cannot
be executed lazily (which has implications on runtime and RAM usage),
and implementations of some operations could be significantly
improved. We will be adding tools to help visualize and understand
query execution in a user-friendly way, but at the moment the best way
to learn more about it is to ask us or to look at the code.

## How does the atomicity model work? ##

Write atomicity is supported on a per-document basis -- updates to a
single JSON document are guaranteed to be atomic. RethinkDB is
different from other NoSQL systems in that atomic document updates
aren't limited to a small subset of possible operations -- any
combination of operations that can be performed on a single document is
guaranteed to update the document atomically. For example, the user
may wish to update the value of attribute A to a sum of the values of
attributes B and C, increment the value of attribute D by a fixed
number, and append an element to an array in attribute E. All of these
operations can be applied to the document atomically in a single
update operation.

However, RethinkDB does come with some restrictions regarding which
operations can be performed atomically. Operations that cannot be
proven deterministic cannot update the document in an atomic
way. Currently, values obtained by executing JavaScript code, random
values, and values obtained as a result of a subquery
(e.g. incrementing the value of an attribute by the value of an
attribute in a different document) cannot be performed atomically. If
the user runs a query that cannot be executed atomically, by default
RethinkDB will throw an error. The user can choose to set the flag on
the update operation in the client driver to execute the query in a
non-atomic way.

In addition, like most NoSQL systems, RethinkDB does not support
updating multiple documents atomically.

## How are concurrent queries handled? ##

To efficiently perform concurrent query execution RethinkDB implements
block-level multiversion concurrency control (MVCC). Whenever a write
operation occurs while there is an ongoing read, RethinkDB takes a
snapshot of the B-Tree for each relevant shard and temporarily
maintains different versions of the blocks in order to execute read
and write operations concurrently. From the perspective of the
applications written on top of RethinkDB, the system is essentially
lock-free&mdash; you can run an hour-long analytics query on a live system
without blocking any real-time reads or writes.

RethinkDB does take exclusive block-level locks in case multiple
writes are performed on documents that are close together in the
B-Tree. If the contested block is cached in memory, these locks are
extremely short-lived. If the blocks need to be loaded from disk,
these locks take longer. Typically this does not present performance
problems because the top levels of the B-Tree are all cached along with
the frequently used blocks, so in most cases writes can be performed
essentially lock-free.

{% endfaqsection %}

{% faqsection Data storage %}

## How is data stored on disk? ##

The data is organized into B-Trees, and stored on disk using a
log-structured storage engine built specifically for RethinkDB and
inspired by the architecture of BTRFS. The storage engine has a number
of benefits over other available options, including an incremental,
fully concurrent garbage compactor, low CPU overhead and very
efficient multicore operation, a number of SSD optimizations,
instantaneous recovery after power failure, full data consistency in
case of failures, and support for multiversion concurrency control.

The storage engine is used in conjunction with a custom, B-Tree-aware
caching engine which allows file sizes many orders of magnitude
greater than the amount of available memory. As a result, neither the
data nor the index have to fit in RAM &mdash; RethinkDB can operate on
terabytes of data with only a few megabytes of cache space.

## Which file systems are supported? ##

RethinkDB supports most commonly used file systems. However, like most
database systems, RethinkDB uses the DIRECT\_IO mechanism to access
disk efficiently when it is available. Some filesystems cannot be
accessed using the DIRECT\_IO mechanism, but they can still be used
to run RethinkDB with degraded performance. Some encrypted and
journaled filesystems do not allow direct disk access, RethinkDB
might fail to start if you try to place data files onto such
filesystems.

There used to be an issue with running RethinkDB (and other database
systems) on ReiserFS in journaled data mode &mdash; it appears to
cause a crash instead of reporting an error. See
[https://github.com/rethinkdb/rethinkdb/issues/20](https://github.com/rethinkdb/rethinkdb/issues/20).

## How can I perform a backup of my cluster? ##

RethinkDB ships with simple tools to perform a hot backup of a running
cluster. See the [backup instructions](/docs/backup/) for more
details.

{% endfaqsection %}
